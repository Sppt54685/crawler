from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import Select
from selenium.webdriver.common.by import By
import time
import json
from bs4 import BeautifulSoup
import re
import random
import pymongo

#selenium开启火狐无头浏览器
options = webdriver.FirefoxOptions()
options.set_headless()
driver = webdriver.Firefox(firefox_options = options)
#模拟用户登录
driver.get(u'https://www.juzimi.com')
time.sleep(3)
id_input = driver.find_element_by_id('edit-name')
id_input.send_keys('sppt1226')
pw_input = driver.find_element_by_id('edit-pass')
time.sleep(2)
pw_input.send_keys('sppt1226')
time.sleep(5)
submit_click = driver.find_element_by_id('edit-submit')
driver.execute_script('arguments[0].click();',submit_click)
driver.implicitly_wait(1)
cookie_list = driver.get_cookies()

url = 'https://www.juzimi.com'
authors = ['张爱玲','东野圭吾']
counts = 10


#连接mongodb
client = pymongo.MongoClient('mongodb://127.0.0.1:27017')

#第一层循环用于获取目标网页源代码
for author in authors:
    #准备数据库
    db = client.test #“图书馆”（数据库）名为test
    collection = db[author]  #用author命名“书架”（集合）
    #判断集合author_otto是否为空 不是则清空集合
    collist = db.list_collection_names()
    if author in collist:
        print('the collection named ',author, ' already exits!')
        #查询“书架”上是否有“书”（文档），若有则清空
        mydict = collection.find()
        if mydict:
            x  = collection.delete_many({})
            print(x.deleted_count,'个文档已删除')
    #获取最大页码
    RealUrl = url + '/search/node/' + author + '%20type:sentence'
    driver.get(RealUrl)
    html = driver.page_source
    soup = BeautifulSoup(html,'lxml')
    pages = soup.find_all('li',class_='pager-last')
    pages = int(pages[0].text)

    time.sleep(3)
    
    #爬虫工作
    for page in range(pages):
        print(f'{author} page:{page+1}/{pages}')
        try:
            contents = soup.find_all('div',class_ = 'views-field-phpcode-1')
            titles = soup.find_all('div',class_ = 'xqjulistwafo')
            for count in range(counts):
                content = contents[count].text.replace('\n','')
                title = titles[count].text.replace('（全文）', '').replace(author,'').replace('——','')
               #data为书架上的书
                data = {
                    'author':author,
                    'title':title,
                    'juzi':content
                     }
                result = collection.insert_one(data)


                time.sleep(3)                    

            if page < pages:
                RealUrl = url + '/search/node/' + author + '%20type%3Asentence?page=' + format(page+1)
                driver.get(RealUrl)
                html = driver.page_source
                soup = BeautifulSoup(html,'lxml')

        except UnicodeEncodeError:
            if page < pages:
                RealUrl = url + '/search/node/' + author + '%20type%3Asentence?page=' + format(page+1)
                driver.get(RealUrl)
                html = driver.page_source
                soup = BeautifulSoup(html,'lxml')
        except Exception as e:
            print(e)
driver.quit()
