from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import Select
from selenium.webdriver.common.by import By
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import time
import requests
import re
import json 


starttime = time.time()

#设置代理ip
profile = webdriver.FirefoxProfile()
profile.set_preference('network.proxy.type', 1)
profile.set_preference('network.proxy.http', '222.212.88.12')
profile.set_preference('network.proxy.http_port', 32142)
profile.set_preference('network.proxy.ssl', '222.212.88.12')
profile.set_preference('network.proxy.ssl_port', 32142)
profile.update_preferences()
driver = webdriver.Firefox(profile)
#driver.add_argument('headless')
driver.minimize_window()

#用户登录并获取cookie
url='https://www.juzimi.com'
driver.get(u"https://www.juzimi.com")
id_input = driver.find_element_by_id('edit-name')
id_input.send_keys('sppt1226')
#time.sleep(10)
pw_input = driver.find_element_by_id('edit-pass')
pw_input.send_keys('sppt1226')
time.sleep(5)
submit_click = driver.find_element_by_id('edit-submit')
driver.execute_script('arguments[0].click();',submit_click)
time.sleep(10)
#print(driver.current_url)
text = driver.page_source
cookie = driver.get_cookies()
#print(cookie)
jsonCookies = json.dumps(cookie)
with open('juzimipage.json','w') as f:
    f.write(jsonCookies)
str=''
with open('juzimipage.json','r',encoding='utf-8') as f:
    listCookies=json.loads(f.read())
cookie = [item["name"] + "=" + item["value"] for item in listCookies]
cookiestr = '; '.join(item for item in cookie)
#print(cookiestr)
driver.quit()

#设置请求头
proxies={'http':'222.212.88.12:32142','https':'222.212.88.12:32142'}
header = {
'Connection': 'keep-alive',
'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
'Accept-Encoding':'gzip, deflate, br',
'Accept-Language':'zh-CN,zh;q=0.9',
'Cookie': cookiestr,
'Upgrade-Insecure-Requests':'1',
'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.109 Safari/537.36'
}

#关键字
authors = "沈从文 三毛 张爱玲 东野圭吾"
authors = ['李白', '木心', '鲁迅', '王尔德'] + authors.split()
motto2_content = []
motto2_txt = open('motto2.txt', 'w', encoding='utf-8')
#counts计当页motto数
counts = 15
#第一层循环用于获取目标网页源代码
for author in authors:
    motto2_txt.write('\n\n# ' + author +
                    '\n------------------------------------\n')
    Url = url + '/search/node/' + author + '%20type:sentence'
    html = requests.get(url=Url,proxies=proxies,headers=header).text
    #print(html)
    soup = BeautifulSoup(html,'lxml')
    #print(soup)
    pages = soup.find_all('li',class_ = 'pager-last')
    pages = int(pages[0].text)
    #texts = soup.find_all('div', class_ = 'views-field-phpcode-1')
    #print(texts)
    time.sleep(10)
    #第二层保存数据
    for page in range(pages):
        print(f'{author} page:{page}/{pages}')
        try:
            contents = soup.find_all('div',class_ = 'views-field-phpcode-1')
            titles = soup.find_all('span',class_ = 'views-field-field-oriarticle-value')

            for count in range(counts):
                t = motto2.text
                motto2_txt.write(t + '\n\n')
                motto2_content.append(
                    {'author': author,
                     'title': titles[count],
                     'paragraphs': [contents[count]]})
                print(motto2_content[-1])
                #time.sleep(5)
            if page < pages:
                 Url = url + '/search/node/' + author + '%20type%3Asentence?page=' + page
                #soup.find_all('li',class_ = 'pager-next').click()
        except UnicodeEncodeError:
            if page < pages:
                Url = url + '/search/node/' + author + '%20type%3Asentence?page=' + page
                #soup.find_all('li',class_ = 'pager-next').click()
        except Exception as e:
            print(e)
 
driver.close()
motto_txt.close()
