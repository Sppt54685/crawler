from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import Select
from selenium.webdriver.common.by import By
import time
import json
import random
from fake_useragent import UserAgent 
import requests
from bs4 import BeautifulSoup
import pymongo


url = 'https://www.juzimi.com'


def driver_cookie():
    drivercookielist = []
    for num in range(1,4):
        num = str(num)
        with open('juzicookie'+num+'.json','r',encoding='utf-8') as f:
            listCookies=json.loads(f.read())
        drivercookielist.append(listCookies)
    return drivercookielist  


def cookie_list():
    drivercookies = driver_cookie()
    cookielist = []
    for dcookie in drivercookies:
        cookie = [item["name"] + "=" + item["value"] for item in dcookie]
        cookiestr = '; '.join(item for item in cookie)
        cookielist.append(cookiestr)
    return cookielist


def get_header():
    cookiestr = cookie_list()
    ua = UserAgent()
    header = {
        'Connection': 'keep-alive',
        'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'Accept-Encoding':'gzip, deflate, br',
        'Accept-Language':'zh-CN,zh;q=0.9',
        'Cookie': random.choice(cookiestr),
        'Upgrade-Insecure-Requests':'1',
        'User-Agent':ua.random
    }
    return header


def get_pages(keyword):
    pro = ['211.159.171.58:80','163.204.245.64:9999','115.218.218.15:9000','123.55.39.54:53281']
    headers = get_header()
    RealUrl = url + '/search/node/' + keyword + '%20type:sentence'
    html = requests.get(url=RealUrl,proxies={'http':random.choice(pro)},headers=headers).text
    soup = BeautifulSoup(html,'lxml')
    pages = soup.find_all('li',class_ = 'pager-last')
    pages = int(pages[0].text)
    time.sleep(3)
    return pages


def urllist(keyword):
    urllist = set()
    pages = get_pages(keyword)	
    for page in range(pages):
        RealUrl = url + '/search/node/' + keyword + '%20type%3Asentence?page=' + format(page)
        urllist.add(RealUrl)
    return urllist

	
def get_html(keyword):
    dcookiestr = driver_cookie()
    dcookiestr = random.choice(dcookiestr)

    option = webdriver.FirefoxOptions()
    option.set_headless()
    driver = webdriver.Firefox(firefox_options=option)
    driver.get(url)
    driver.delete_all_cookies()
    for cookie in dcookiestr:
        driver.add_cookie(cookie)
		
    html_list = []
    visitedUrl = []
    url_set = urllist(keyword)
    url_list = list(url_set)
    counts = len(url_list)
    for count in range(counts):
        savedUrl = random.choice(url_list)
        while(savedUrl in visitedUrl):
            savedUrl = random.choice(url_list)
        driver.get(savedUrl)
        html = driver.page_source
        soup = BeautifulSoup(html,'lxml')
        html_list.append(soup)
#        html_txt = open(keyword + '.txt', 'w', encoding='utf-8')
#        html_txt.write(html  + '\n\n-------------------\n\n')
        time.sleep(1)
        visitedUrl.append(savedUrl)
        time.sleep(2)
    visitedUrl = set(visitedUrl)
    if url_set==visitedUrl:
        print('目标网页皆已保存')
    driver.quit()
    return html_list	

	
def crawler(keyword):
    html_list = get_html(keyword)
    pages = len(html_list)
    page = 1
    counts = 10
    client = pymongo.MongoClient('mongodb://127.0.0.1:27017')
    db = client.test
    collection = db[keyword]
    collist = db.list_collection_names()
    if keyword in collist:
        print(keyword,'already exits')
        mydict = collection.find()
        if mydict:
            x = collection.delete_many({})
            print(x.deleted_count,'个文档已删除')
    for soup in html_list:
        print(f'{keyword} page:{page}/{pages}')
        contents = soup.find_all('div',class_ = 'views-field-phpcode-1')
        titles = soup.find_all('div',class_ = 'xqjulistwafo')
        for count in range(counts):
            content = contents[count].text.replace('\n','')
            title = titles[count].text.replace('（全文）', '').replace(keyword,'').replace('——','')
            data = {
                'author':keyword,
                'title':title,
                'juzi':content
            }
            insert_mongo = collection.insert_one(data)
            page = page+1
            time.sleep(2)                 
    if page==pages:
        print('目标网页皆已保存')

	
if __name__ == '__main__':
    crawler('张爱玲')
